



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.6.0">
    
    
      
        <title>2. Classical ML and NLP - IEEE Machine Learning Bootcamp</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.1b62728e.css">
      
      
    
    
      <script src="../assets/javascripts/modernizr.268332fc.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
    
      
    
    
  </head>
  
    <body dir="ltr">
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#what-is-classical-machine-learning" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href=".." title="IEEE Machine Learning Bootcamp" class="md-header-nav__button md-logo">
          
            <i class="md-icon">î Œ</i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              IEEE Machine Learning Bootcamp
            </span>
            <span class="md-header-nav__topic">
              
                2. Classical ML and NLP
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main" role="main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href=".." title="IEEE Machine Learning Bootcamp" class="md-nav__button md-logo">
      
        <i class="md-icon">î Œ</i>
      
    </a>
    IEEE Machine Learning Bootcamp
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../0. Setup/" title="Getting Started" class="md-nav__link">
      Getting Started
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../1. Basics/" title="1. Basics" class="md-nav__link">
      1. Basics
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        2. Classical ML and NLP
      </label>
    
    <a href="./" title="2. Classical ML and NLP" class="md-nav__link md-nav__link--active">
      2. Classical ML and NLP
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#what-is-classical-machine-learning" class="md-nav__link">
    What is Classical Machine Learning?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#scikit-learn" class="md-nav__link">
    scikit-learn
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-challenge" class="md-nav__link">
    The Challenge
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-dataset" class="md-nav__link">
    The Dataset
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#classification-and-clustering" class="md-nav__link">
    Classification and Clustering
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#classifications-and-clustering-algorithms" class="md-nav__link">
    Classifications and Clustering Algorithms
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#support-vector-machines" class="md-nav__link">
    Support Vector Machines
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kmeans" class="md-nav__link">
    KMeans
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#natural-language-processing" class="md-nav__link">
    Natural Language Processing
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#feature-engineering-and-nlp" class="md-nav__link">
    Feature Engineering and NLP
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bag-of-words" class="md-nav__link">
    Bag of Words
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tf-idf" class="md-nav__link">
    TF-IDF
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#nlp-with-sklearn" class="md-nav__link">
    NLP with sklearn
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#loading-data" class="md-nav__link">
    Loading Data
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-cleaning" class="md-nav__link">
    Data Cleaning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#feature-creation" class="md-nav__link">
    Feature Creation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../3. Neural Networks and Computer Vision/" title="3. Neural Networks and Computer Vision" class="md-nav__link">
      3. Neural Networks and Computer Vision
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../Additional Resources/" title="Additional Resources" class="md-nav__link">
      Additional Resources
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../Extras/" title="Extras" class="md-nav__link">
      Extras
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#what-is-classical-machine-learning" class="md-nav__link">
    What is Classical Machine Learning?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#scikit-learn" class="md-nav__link">
    scikit-learn
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-challenge" class="md-nav__link">
    The Challenge
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-dataset" class="md-nav__link">
    The Dataset
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#classification-and-clustering" class="md-nav__link">
    Classification and Clustering
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#classifications-and-clustering-algorithms" class="md-nav__link">
    Classifications and Clustering Algorithms
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#support-vector-machines" class="md-nav__link">
    Support Vector Machines
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kmeans" class="md-nav__link">
    KMeans
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#natural-language-processing" class="md-nav__link">
    Natural Language Processing
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#feature-engineering-and-nlp" class="md-nav__link">
    Feature Engineering and NLP
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bag-of-words" class="md-nav__link">
    Bag of Words
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tf-idf" class="md-nav__link">
    TF-IDF
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#nlp-with-sklearn" class="md-nav__link">
    NLP with sklearn
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#loading-data" class="md-nav__link">
    Loading Data
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-cleaning" class="md-nav__link">
    Data Cleaning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#feature-creation" class="md-nav__link">
    Feature Creation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>2. Classical ML and NLP</h1>
                
                <h2 id="what-is-classical-machine-learning"><strong>What is Classical Machine Learning?</strong></h2>
<p>In this bootcamp, when we refer to Classical Machine Learning Algorithms, we simply mean algorithms that are not based on Deep Learning or Neural Network based learning methods. These algorithms have been used for decades, far before the current hype of Machine Learning and Artificial Intelligence .</p>
<p>Some examples of Classical Machine Learning Algorithms include but are not limited to:</p>
<p><strong>Supervised Learning</strong>: Naive Bayes, Linear Regression, Logistic Regression, Ridge Regression, K Nearest Neighbors, <strong>Support Vector Machine</strong></p>
<p><strong>Unsupervised Learning</strong>: Mean-shift, DBSCAN, <strong>K-Means</strong></p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p><strong><em>DEFINITION DETOUR</em></strong>: Unsupervised and Supervised Learning</p>
<p><strong>Supervised</strong>: Supervised learning is unique in that it has training data. The model will learn from labeled data in order to predict something, usually in the form of classification or regression.</p>
<p><strong>Unsupervised</strong>:  Unsupervised learning is when the algorithm attempts to create structure from unlabeled data. This is usually in the form of clustering, but can take other forms, such as learning representations of data.</p>
</div>
<p>Since Classical Machine Learning Algorithms are often outclassed by Deep Learning algorthims, <strong><em>why should we even use them?</em></strong> The answer is more clear than you think.</p>
<ol>
<li>
<p><strong><em>Simplicity</em></strong>. Classical Machine Learning Methods are often easier to explain and more computationally efficient that Deep Learning Based Approaches, allowing them to be deployed much easier and cheaper than their neural network-based counterparts. In addition, most classical algorithms run directly on the CPU, voiding the need for more costly GPU's. </p>
</li>
<li>
<p><strong><em>Perform Better with Less Data</em></strong>. Classical Machine Learning algorithms don't need as much data to get good predictions, and in many cases can perform better than neural networks with limited data.</p>
</li>
</ol>
<h2 id="scikit-learn"><strong>scikit-learn</strong></h2>
<p>Scikit-learn includes off the shelf machine learning algorithms and tools, including all of the algorithms we are using in this workshop. Scikit-learn also includes many tools that we can use in our Natural Language processing workflows, simplifying a lot of our workflows. </p>
<p><img alt="scikit" src="../images/scikit.png" /></p>
<h2 id="the-challenge"><strong>The Challenge</strong></h2>
<p>You recently got fired from you job as a data scientist from your job as because they discovered that you lied on your resume and couldn't implement linear regression from scratch. You decide to pivot and work for a newspaper, who is trying to figure out the category of each news article to deliver tailored ads. In addition, they want to create a recommender system to recommend articles to readers. How do you do this?</p>
<p><a href="https://drive.google.com/file/d/14_T2axX3wwC0n2BlIrHgN-nUynvqIwns/view?usp=sharing" download>Click to Download Starter Code</a></p>
<p><a href="https://drive.google.com/file/d/1WPnMP0acNLXFW1gawNecAAYn01pORRZ2/view?usp=sharing" download>Click to Solution Code</a></p>
<h3 id="the-dataset">The Dataset</h3>
<p><div style="text-align:center" ><img width="70%" height="70%" src="https://images.pexels.com/photos/265642/pexels-photo-265642.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260" /></div></p>
<p>The dataset that we are using is called the <strong>20 Newsgroups Dataset</strong> which contains ~20,000 news documents spread across 20 different types of newsgroups. This dataset is made out of text documents, with each having a corresponding classification for each newsgroups, or category for each document. If given a new document, how can we predict what newsgroup it is in, and how can we find like documents as well?</p>
<p><center>
<table>
<tr>
<td>comp.graphics<br>comp.os.ms-windows.misc<br>comp.sys.ibm.pc.hardware<br>comp.sys.mac.hardware<br>comp.windows.x</td>
<td>rec.autos<br>rec.motorcycles<br>rec.sport.baseball<br>rec.sport.hockey</td>
<td>sci.crypt<br>sci.electronics<br>sci.med<br>sci.space</td>
</tr><tr>
<td>misc.forsale</td>
<td>talk.politics.misc<br>talk.politics.guns<br>talk.politics.mideast</td>
<td>talk.religion.misc<br>alt.atheism<br>soc.religion.christian</td>
</tr>
</table>
</center></p>
<h2 id="classification-and-clustering">Classification and Clustering</h2>
<p>To complete the above task, we can use two basic tasks in machine learning - <strong>classification and clustering</strong>. </p>
<p>The task of <strong>classification</strong> is simple, given a new sample, how can we classify what category this sample belongs to by using past data? In the previous workshop, we used regression, which was a supervised task, in which we had training data, which our algorithm used to create a line from which we can predict continuous values. We created <strong>labels</strong>, which was a category that each corresponding sample belonged to. However, instead of predicting the value of something given a sample, we want to tell whether this sample belongs to a classification or not. In general the goal of classification is to reduce our errors, or reduce the amount of times that our model will classify a sample incorrectly.</p>
<p>We showed classification in the last workshop, where we classified our samples to predict whether these samples were over $200,000 or not, but in this workshop we will go a bit deeper and discuss different algorithms and methods that we can use to classify our datasets. </p>
<p><img alt="scikit" src="../images/classification.png" /></p>
<p><strong>Clustering</strong> is a bit different. For clustering, the goal is to take data and separate them into like groups. Clustering is a mainly unsupervised task where we simply group a dataset into <strong>clusters</strong>, where the samples within each cluster are most similar to each other. We will talk later about how we can decide whether the samples in these clusters are most similar to each other, but in general, these are used to separate data and find inherent differences in data without labels. </p>
<p>Just as the last workshop, we can set the objective function for this task, as minimizing <span><span class="MathJax_Preview">J_{clust}</span><script type="math/tex">J_{clust}</script></span>, where for each J for each cluster such that </p>
<div>
<div class="MathJax_Preview"> J_{clust} = J_{1} + J_{2} + \cdots + J_{n}  </div>
<script type="math/tex; mode=display"> J_{clust} = J_{1} + J_{2} + \cdots + J_{n}  </script>
</div>
<p>And each <span><span class="MathJax_Preview">J_{n}</span><script type="math/tex">J_{n}</script></span>, given as </p>
<div>
<div class="MathJax_Preview"> J_{n} = (1/N) \sum_{i \in G_{j}}^{} \|x_{i} - z_{j}\| </div>
<script type="math/tex; mode=display"> J_{n} = (1/N) \sum_{i \in G_{j}}^{} \|x_{i} - z_{j}\| </script>
</div>
<p>Where, <span><span class="MathJax_Preview">G_{j}</span><script type="math/tex">G_{j}</script></span> is each group, and <span><span class="MathJax_Preview">z_{j}</span><script type="math/tex">z_{j}</script></span> is each of the group representatives, for each group with index j.</p>
<h2 id="classifications-and-clustering-algorithms">Classifications and Clustering Algorithms</h2>
<h3 id="support-vector-machines">Support Vector Machines</h3>
<p>Support Vector Machine, or <strong>SVM</strong> is a relatively new, yet powerful classifier. The workings of an SVM classifier is quite simple. It tries to create a <strong>hyperplane</strong> in the data space such that it can separate this entire space, with all samples on one side of the hyperplane as one class, and the samples on the other side of this hyperplane as the other class.</p>
<p>To give a geometric representation, the hyperplane below can be illustrated in an <span><span class="MathJax_Preview">\mathbb{R}^2</span><script type="math/tex">\mathbb{R}^2</script></span> and <span><span class="MathJax_Preview">\mathbb{R}^3</span><script type="math/tex">\mathbb{R}^3</script></span> feature space. In <span><span class="MathJax_Preview">\mathbb{R}^2</span><script type="math/tex">\mathbb{R}^2</script></span>, the hyperplane can be defined as a line, and above that a plane to denote the classification boundary in <span><span class="MathJax_Preview">\mathbb{R}^3</span><script type="math/tex">\mathbb{R}^3</script></span>. </p>
<p><img alt="SVM Hyperplanes" src="../images/svmplanes.png" /></p>
<p>Although we won't go to deep into the theory behind how we find this hyperplane, we will go a bit into how we find this hyperplane. 
Support Vector Machine gets its name from the support vectors that the algorithm includes. Support vectors are the points that lie closest to the hyperplane, and are therefore the most difficult to classify. The hyperplane is defined by this very small subset of these support vectors, and we want to maximize the margin between the support vectors and the hyperplane. This is given by an analogy of maximizing the street between the support vectors on each side of the hyperplane, where the edges of the street are defined by the support vectors, and the center of the street defined as the hyperplane. </p>
<p>We can give the object function for the margin in that we want to maximize the margin from the hyperplane to the support vectors, where we can minimize the margin, </p>
<div>
<div class="MathJax_Preview"> maximize \: \frac{2}{{\|w\|}} </div>
<script type="math/tex; mode=display"> maximize \: \frac{2}{{\|w\|}} </script>
</div>
<p><img alt="SVM Margin" src="../images/margin.png" /></p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p><strong>What is the kernel trick?</strong></p>
<p>This is a common question asked in many ML interviews, and is fairly useful when trying to create a proper hyperplane. <strong>Essentially, the kernel trick is transforming your data to a higher dimension in order to create a hyperplane that can linearly separate the samples.</strong> For instance, in the image below, we obviously can't create a line that can intersect these data points to separate the two classes. But we can use a symmetric function, or kernel, to transform this data. </p>
<p>The Radial Basis Function, or RBF, is commonly used as a kernel function in addition to Gaussian Filters as well.</p>
<p><img alt="Kernel Trick" src="https://www.researchgate.net/profile/Husam_Al-Behadili/publication/305284381/figure/fig1/AS:384096221057024@1468587083237/Kernel-trick-By-transforming-the-original-space-left-into-a-space-of-increased.png" /></p>
<p>Learn more about kernel functions here<sup id="fnref:1"><a class="footnote-ref" href="#fn:1" rel="footnote">1</a></sup><sup id="fnref:2"><a class="footnote-ref" href="#fn:2" rel="footnote">2</a></sup></p>
</div>
<h3 id="kmeans">KMeans</h3>
<p>Wow, we were so into SVM classification that we almost forgot about the reccomender system. Lets try not to get that, as we definitely wouldn't want to get fired from this job as well. Let's start this problem by doing a quick case study on how recommender systems are typically implemented in industry, most notably how <strong>Netflix</strong> recommends content to its viewers.</p>
<p><strong>K-Means</strong> is a powerful heuristic that we can use to cluster our datasets. K-Means works all around its <strong>cluster representatives</strong>. Clusters are defined by the centroids, or center of each cluster with each sample being added to a cluster depending on the closest centroid to that respective sample. </p>
<p>We can initialize kmeans by randomly selecting k points as our initial centroids, and then creating clusters based on the closest points to those centroids. We can then set the centroid to the average of all the points in the cluster, and then reiterate until the clusters do not change.</p>
<p><img alt="Kmeans animation" src="../images/kmeans.gif" /></p>
<p><center>K-means psuedocode</center></p>
<blockquote>
<p>given a list of N vectors <span><span class="MathJax_Preview">[x_1 \; \cdots \;  x_N]</span><script type="math/tex">[x_1 \; \cdots \;  x_N]</script></span> and an initial list of k group representative vectors <span><span class="MathJax_Preview">[z_1 \; \cdots \;  z_k]</span><script type="math/tex">[z_1 \; \cdots \;  z_k]</script></span></p>
<p>repeat until convergence:</p>
<ol>
<li>
<p>Partition the vectors into k groups. For each vector i = 1 ... N, assign <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> to the group associated with the nearest representative.</p>
</li>
<li>
<p>Update representatives. For each group j = 1 .. k, set <span><span class="MathJax_Preview">z_j</span><script type="math/tex">z_j</script></span> to be the mean of the vectors in group j.</p>
</li>
</ol>
</blockquote>
<div class="admonition faq">
<p class="admonition-title">Faq</p>
<p>Question: How can you know what is a good number of clusters to split your data into?</p>
<p>Just like the nature of K-Means, we can actually use another heuristic to find the correct value for k, also known as the <strong>elbow method</strong>. </p>
<p>To do this, we can run K-means for different k-values, and plot the final <span><span class="MathJax_Preview">J_{clust}</span><script type="math/tex">J_{clust}</script></span> value for each k. The graph should look like an angle with a large decrease in <span><span class="MathJax_Preview">J_{clust}</span><script type="math/tex">J_{clust}</script></span> values, with the <span><span class="MathJax_Preview">J_{clust}</span><script type="math/tex">J_{clust}</script></span> vlaues leveling off. This heuristic says that we should pick the k-value at the "elbow" of this angle, so we don't have too many clusters, but also not too little that our <span><span class="MathJax_Preview">J_{clust}</span><script type="math/tex">J_{clust}</script></span> value is too high. </p>
<p><div style="text-align:center" ><img width="70%" height="70%" src=" https://www.scikit-yb.org/en/latest/_images/elbow-1.png" /></div></p>
</div>
<p>Just keep in mind that we call this a heuristic because this algorithm won't always find the most optimal <span><span class="MathJax_Preview">J_{clust}</span><script type="math/tex">J_{clust}</script></span> each time we run the algorithm. Depending on how K-Means is initialized, each time we cluster our dataset we will get different clusters.</p>
<div class="admonition faq">
<p class="admonition-title">Faq</p>
<p><strong><em>What's the difference between K-Means and K-Nearest Neighbors?</em></strong></p>
<p>This is a pretty common question, and even a common interview question, as it's quite easy to confuse one with another, even though they really aren't similar at all. The only similarity between the two is one of the hyperparameters - K. In K-Means, k defines how many groups you want your data to be clustered into. K-Means as we have shown earlier is a clustering algorithm.</p>
<p>On the other had, K-Nearest Neighbors is a voting-based classification algorithm. It works by taking the the k number of samples closest to the sample you are trying to classify. Using those K samples, the unknown sample is given the classification of the class with the most labels out of those K samples.</p>
<p><img alt="scikit" src="../images/knn.png" /></p>
<p><strong>In short, K-Means is an unsupervised classification model, while K-Nearest Neighbors is a supervised classification model.</strong></p>
</div>
<h2 id="natural-language-processing">Natural Language Processing</h2>
<p>In the past workshop, we were dealing with almost exclusively numerical data. This made things pretty easy for us. This allowed us to put these numbers into vectors, which could easily work with our machine learning algorithms. However, with text data, if you wanted to use it with machine learning, we have to answer one main question</p>
<p><strong><em>How can we turn our text data into features that our machine learning model can use?</em></strong></p>
<p>More specifically, how can we derive computational methods to derive meaning or get analytics from text data? Natural Language Processing is a very broad field that intersects the field of machine learning greatly, but we will be using a few NLP methods to make high performing NLP Models. </p>
<h2 id="feature-engineering-and-nlp">Feature Engineering and NLP</h2>
<p><strong>Feature Engineering</strong> is the process of making Features from our data. Making proper features is essential for any Machine Learning workflow, especially for applications of NLP. </p>
<h3 id="bag-of-words">Bag of Words</h3>
<p>One popular way to create proper features for NLP applications is by using the <strong>Bag of Words</strong> representation of text data. In bag of words, the order of words and grammar is unimportant, as we only care about the amount of times a words has appeared in a certain text. </p>
<p>When using bag of words, a matrix would be created with every row is a specific word with the columns being the count of the word or other measurement. Essentially, the array is made of "bins" where each measurement is stored. For example, if we wanted to do a bag of words model with counts on this <strong>corpus</strong> (a corpus is essentially your full collection or dataset of text)</p>
<ol>
<li>
<p>This is the first document</p>
</li>
<li>
<p>This document is the second document</p>
</li>
<li>
<p>And this is the third one.</p>
</li>
<li>
<p>Is this the first document?</p>
</li>
</ol>
<p>Your final bag of words representation using counts, or your <strong>Count Vectorizer</strong>, would look like this:</p>
<table>
<thead>
<tr>
<th>and</th>
<th>document</th>
<th>first</th>
<th>is</th>
<th>one</th>
<th>second</th>
<th>the</th>
<th>third</th>
<th>this</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>0</td>
<td>2</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You might think that <strong>when using the bag of words representation, you will be removing a lot of contextual and grammatical data about text</strong>. This is true, but for many large scale text applications such as the one we are doing here, specific grammar and other traits of sentences are irrelevant. For example, if you wanted to make your own version of the UCSD Spam Quarantine, you might find that many emails labeled as spam, would likely have very high frequency of certain words, likely "buy", "free", or other similar words. </p>
<p>On the other hand, grammar might not be as important to the classification as certain words may be. However, this does not mean that not creating features from grammar is a bad idea, as it is very important in many subsections of NLP such as sentiment analysis and speech translation.</p>
</div>
<h3 id="tf-idf">TF-IDF</h3>
<p>On the other hand, <strong>using a count vectorizer isn't always the best method of creating a bag of words representation for your text</strong>. For example, what if one of your text samples is much longer than others which is the case for many datasets? Obviously the counts would be higher, but these words with higher counts would be weighed higher in your model, therefore causing this specific sample to skew your model. We can alter the count vectorization method by instead using a frequency method, where your bag of words representation is based on proportions instead of raw counts.</p>
<p>The most common way to do this is through using term frequency - inverse document frequency method, or <strong>TF-IDF</strong>. </p>
<p>Your term frequency can be calculated as:</p>
<p><sub>$$TF(t) = \text{(# of times word t appears in a document) / (Total # of words in the document)} $$ </sub></p>
<p>IDF calculated as:</p>
<p><sub>$$ IDF(t) = \text{log(Total # of documents / # of documents with word t in it)} $$ </sub></p>
<p><strong>And you final TF-IDF for each word calculated as:</strong></p>
<div>
<div class="MathJax_Preview">TF(t) \cdot IDF(t)</div>
<script type="math/tex; mode=display">TF(t) \cdot IDF(t)</script>
</div>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p><strong>Occam's razor</strong></p>
<p>You may have heard Occam's Razor before, which can be paraphrased as "The simplest solution is most likely the right one." This saying can be true in any discipline in engineering, and is especially true with machine learning models. Higher complexity and unneeded features can cause your model to perform, worse, as yur model can start to weigh in these unnecessary features.  Additional complexity to machine learning models also adds extra computational cost, meaning higher actual cost when your models are deployed. <strong>This reason is why data cleaning and reducing features is necessary to create well performing ML models.</strong></p>
<p><div style="text-align:center" ><img width="40%" height="40%" src="https://149366099.v2.pressablecdn.com/wp-content/uploads/2017/05/occams-razor-white.jpg" /></div></p>
<p><strong>Always try to cut out complexity from your model</strong></p>
</div>
<h2 id="nlp-with-sklearn">NLP with sklearn</h2>
<h3 id="loading-data">Loading Data</h3>
<p>This part of the workshop is pretty easy compared to the last time in terms of importing data (last time was easy too, but this is even easier ðŸ˜Š).</p>
<p>sklearn includes some nice functions to load this dataset. Simply put, we can use the below functions to import the dataset and filter out extraneous data, such as the article headers and footers. After that, make sure to load this data into a dataframe to make use of the nice filtering functions in pandas. </p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="n">newsgroups</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span><span class="n">remove</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;headers&#39;</span><span class="p">,</span> <span class="s1">&#39;footers&#39;</span><span class="p">,</span> <span class="s1">&#39;quotes&#39;</span><span class="p">))</span>
</pre></div>
</td></tr></table>

<h3 id="data-cleaning">Data Cleaning</h3>
<p>In addition, when creating features from text, there may likely be words that add little to nothing to the meaning of a text, for example words like "for", "and", "the", or "a". We can remove these <strong>stopwords</strong> from our text to help clean our text from such useless features that can add to the complexity of our model. One library that we can use for this is <code class="codehilite"><span class="err">nltk</span></code>, which stands for <b>N</b>atural <b>L</b>anguage <b>T</b>ool<b>k</b>it</p>
<p>We first have to download these stopwords, but with a bit of tricky use of the <code class="codehilite"><span class="err">df.apply</span></code> and <code class="codehilite"><span class="err">lambda</span></code> functions, we can remove these stop words from our data. </p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">Dataframe</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">()</span>
</pre></div>
</td></tr></table>

<p>In addition, for a lot of our text data, we want to remove extraneous symbols and other things that could affect our features. </p>
<h3 id="feature-creation">Feature Creation</h3>
<p>Now that we have removed unwanted text, let's now create features from our text data set. As explained earlier, we can use the bag of words representation with TF-IDF to create features that are independent of text size. This is done easily through <code class="codehilite"><span class="err">TfidfVectorizer()</span></code> in the <code class="codehilite"><span class="err">sklearn.feature_extraction.text</span></code> module.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>CS 229: Kernels <a href="https://see.stanford.edu/Course/CS229/39">https://see.stanford.edu/Course/CS229/39</a>&#160;<a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>Wikipedia: Kernel (Statistics)<a href="https://en.wikipedia.org/wiki/Kernel_(statistics)">https://en.wikipedia.org/wiki/Kernel_(statistics)</a>&#160;<a class="footnote-backref" href="#fnref:2" rev="footnote" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>
                
                  
                
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../1. Basics/" title="1. Basics" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                1. Basics
              </span>
            </div>
          </a>
        
        
          <a href="../3. Neural Networks and Computer Vision/" title="3. Neural Networks and Computer Vision" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                3. Neural Networks and Computer Vision
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.808e90bb.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>